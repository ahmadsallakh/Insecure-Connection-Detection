{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374027d3",
   "metadata": {},
   "source": [
    "# KDD Cup 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b4a8b1",
   "metadata": {},
   "source": [
    "This dataset contains information required to determine whether a connection attempt to a certain website is safe or not. In this document, we will build a Machine Learning model to determine for us whether the connection attempt is secure or not.\n",
    "\n",
    "First step is to import all the libraries that will be used in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48496f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09761bc",
   "metadata": {},
   "source": [
    "# Data Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e01b0",
   "metadata": {},
   "source": [
    "Since the size of the given data is very large, if we try to read the data, we will face memory issues. To avoid that, we should split the data into chunks first. Because we don't have a dedicated header row for our data, we will use the 'header()' parameter with the value 'None'. We should also use 'on_bad_lines()' parameter to avoid issues when reading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe008d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv',header=None,chunksize=10000,on_bad_lines='skip',na_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea75f6bf",
   "metadata": {},
   "source": [
    "Now that we have read our data, we should assign its chuncks into a new dataset so we can access it. We do so by using the 'pandas.concat()' function. We can also use 'ignore_index=True' if we don't want to keep the index labels of the data being concatenated, but since our new dataset is empty, we don't have to use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8099ab8d",
   "metadata": {},
   "source": [
    "Important Note: For this data, we used 'na_filter=False', which makes our code no longer parse empty strings as 'Null' values. But what if we have empty strings, wouldn't that ruin our model? Yes, but since we've checked for missing data in our dataset and we found none, we can use 'na_filter()' parameter to speed up the reading speed of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c88b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f76c7a8",
   "metadata": {},
   "source": [
    "#### We can get a sample of our data using the 'head()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42c42d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>215</td>\n",
       "      <td>45076</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>162</td>\n",
       "      <td>4528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>236</td>\n",
       "      <td>1228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>233</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1     2   3    4      5   6   7   8   9   ...  32   33   34    35  \\\n",
       "0   0  tcp  http  SF  215  45076   0   0   0   0  ...   0  0.0  0.0  0.00   \n",
       "1   0  tcp  http  SF  162   4528   0   0   0   0  ...   1  1.0  0.0  1.00   \n",
       "2   0  tcp  http  SF  236   1228   0   0   0   0  ...   2  1.0  0.0  0.50   \n",
       "3   0  tcp  http  SF  233   2032   0   0   0   0  ...   3  1.0  0.0  0.33   \n",
       "4   0  tcp  http  SF  239    486   0   0   0   0  ...   4  1.0  0.0  0.25   \n",
       "\n",
       "    36   37   38   39   40       41  \n",
       "0  0.0  0.0  0.0  0.0  0.0  normal.  \n",
       "1  0.0  0.0  0.0  0.0  0.0  normal.  \n",
       "2  0.0  0.0  0.0  0.0  0.0  normal.  \n",
       "3  0.0  0.0  0.0  0.0  0.0  normal.  \n",
       "4  0.0  0.0  0.0  0.0  0.0  normal.  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd60697",
   "metadata": {},
   "source": [
    "# 1- Missing Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70c165",
   "metadata": {},
   "source": [
    "To check the amount of Null values in each column, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb9b94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "16    0\n",
       "17    0\n",
       "18    0\n",
       "19    0\n",
       "20    0\n",
       "21    0\n",
       "22    0\n",
       "23    0\n",
       "24    0\n",
       "25    0\n",
       "26    0\n",
       "27    0\n",
       "28    0\n",
       "29    0\n",
       "30    0\n",
       "31    0\n",
       "32    0\n",
       "33    0\n",
       "34    0\n",
       "35    0\n",
       "36    0\n",
       "37    0\n",
       "38    0\n",
       "39    0\n",
       "40    0\n",
       "41    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e88d4",
   "metadata": {},
   "source": [
    "# 2- Duplicates:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef584d",
   "metadata": {},
   "source": [
    "To check the amount of duplicated rows in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f266233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3823439"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa7a9f7",
   "metadata": {},
   "source": [
    "Even though we have duplicated lines in our dataset, we won't drop them just so we can practice dealing with large data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f290b",
   "metadata": {},
   "source": [
    "# 3- Mixed Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc0a66",
   "metadata": {},
   "source": [
    "Now, let's check if we have mixed data in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9c2cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################### DATASET INFO ######################\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898430 entries, 0 to 4898429\n",
      "Data columns (total 42 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   0       int64  \n",
      " 1   1       object \n",
      " 2   2       object \n",
      " 3   3       object \n",
      " 4   4       int64  \n",
      " 5   5       int64  \n",
      " 6   6       int64  \n",
      " 7   7       int64  \n",
      " 8   8       int64  \n",
      " 9   9       int64  \n",
      " 10  10      int64  \n",
      " 11  11      int64  \n",
      " 12  12      int64  \n",
      " 13  13      int64  \n",
      " 14  14      int64  \n",
      " 15  15      int64  \n",
      " 16  16      int64  \n",
      " 17  17      int64  \n",
      " 18  18      int64  \n",
      " 19  19      int64  \n",
      " 20  20      int64  \n",
      " 21  21      int64  \n",
      " 22  22      int64  \n",
      " 23  23      int64  \n",
      " 24  24      float64\n",
      " 25  25      float64\n",
      " 26  26      float64\n",
      " 27  27      float64\n",
      " 28  28      float64\n",
      " 29  29      float64\n",
      " 30  30      float64\n",
      " 31  31      int64  \n",
      " 32  32      int64  \n",
      " 33  33      float64\n",
      " 34  34      float64\n",
      " 35  35      float64\n",
      " 36  36      float64\n",
      " 37  37      float64\n",
      " 38  38      float64\n",
      " 39  39      float64\n",
      " 40  40      float64\n",
      " 41  41      object \n",
      "dtypes: float64(15), int64(23), object(4)\n",
      "memory usage: 1.5+ GB\n",
      "\n",
      "####################################################\n",
      "\n",
      "####################################################\n",
      "\n",
      "####################################################\n",
      "\n",
      "0 integer \t\tUniques: 9883\n",
      "1 string \t\tUniques: 3\n",
      "2 string \t\tUniques: 70\n",
      "3 string \t\tUniques: 11\n",
      "4 integer \t\tUniques: 7195\n",
      "5 integer \t\tUniques: 21493\n",
      "6 integer \t\tUniques: 2\n",
      "7 integer \t\tUniques: 3\n",
      "8 integer \t\tUniques: 6\n",
      "9 integer \t\tUniques: 30\n",
      "10 integer \t\tUniques: 6\n",
      "11 integer \t\tUniques: 2\n",
      "12 integer \t\tUniques: 98\n",
      "13 integer \t\tUniques: 2\n",
      "14 integer \t\tUniques: 3\n",
      "15 integer \t\tUniques: 93\n",
      "16 integer \t\tUniques: 42\n",
      "17 integer \t\tUniques: 3\n",
      "18 integer \t\tUniques: 10\n",
      "19 integer \t\tUniques: 1\n",
      "20 integer \t\tUniques: 2\n",
      "21 integer \t\tUniques: 2\n",
      "22 integer \t\tUniques: 512\n",
      "23 integer \t\tUniques: 512\n",
      "24 floating \t\tUniques: 96\n",
      "25 floating \t\tUniques: 87\n",
      "26 floating \t\tUniques: 89\n",
      "27 floating \t\tUniques: 76\n",
      "28 floating \t\tUniques: 101\n",
      "29 floating \t\tUniques: 95\n",
      "30 floating \t\tUniques: 72\n",
      "31 integer \t\tUniques: 256\n",
      "32 integer \t\tUniques: 256\n",
      "33 floating \t\tUniques: 101\n",
      "34 floating \t\tUniques: 101\n",
      "35 floating \t\tUniques: 101\n",
      "36 floating \t\tUniques: 76\n",
      "37 floating \t\tUniques: 101\n",
      "38 floating \t\tUniques: 100\n",
      "39 floating \t\tUniques: 101\n",
      "40 floating \t\tUniques: 101\n",
      "41 string \t\tUniques: 23\n"
     ]
    }
   ],
   "source": [
    "def info():\n",
    "\n",
    "    print(\"\\n################### DATASET INFO ######################\\n\")\n",
    "\n",
    "    data.info()  \n",
    "    \n",
    "    print(\"\\n####################################################\")\n",
    "    print(\"\\n####################################################\")\n",
    "    print(\"\\n####################################################\\n\")\n",
    "\n",
    "    i=0\n",
    "    for column in data.columns:\n",
    "        \n",
    "        print(i,pd.api.types.infer_dtype(data[column]),\n",
    "              \"\\t\\tUniques:\",data.iloc[:,i].nunique())\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1105c8df",
   "metadata": {},
   "source": [
    "# 4- Dependent and Independent Variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb4798",
   "metadata": {},
   "source": [
    "There are no mixed data in our columns, but we have string-type data which we have to encode. But first, let's allocate our dependent and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1df1c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b8e62",
   "metadata": {},
   "source": [
    "'X' variable represents our dataset features, and 'y' variable represents our target. We stored all the columns except the last column in 'X', and the last column in our data in 'y'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c1201",
   "metadata": {},
   "source": [
    "# 5- LabelEncoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d0bd1",
   "metadata": {},
   "source": [
    "We have 3 string-type columns in our dependent variable, and 1 string-type column in our independent variable. Machine Learning models cannot deal with string-type data. Therefore, we have to convert any string-type data into numerical data. To do so, we can use the function 'sklearn.preprocessing.LabelEncoder()', which converts all the data found in the given columns to numerical data.\n",
    "\n",
    "For the columns in the dependent variable, we will have to use 'LabelEncoder()' and an additional encoding method to make the encoded data meaningful for our model. For the column in our independent variable, we will only have to use 'LabeleEncoder()' because it's our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd6ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "X[:,1] = labelencoder.fit_transform(X[:,1]).astype(float)\n",
    "X[:,2] = labelencoder.fit_transform(X[:,2]).astype(float)\n",
    "X[:,3] = labelencoder.fit_transform(X[:,3]).astype(float)\n",
    "\n",
    "y = labelencoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa00b01",
   "metadata": {},
   "source": [
    "You can notice that we parsed the encoded 'X' columns as float. This is not necessary since 'CatBoostEncoder()' does that anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c6154",
   "metadata": {},
   "source": [
    "# 6- CatBoostEncoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ca58b",
   "metadata": {},
   "source": [
    "For datasets of this volume, it takes too long to process its data, so if we want to add more columns to the data, it will take even longer. For this reason, we will use 'CatBoostEncoder()' to encode our data instead of using 'pandas.get_dummies()'.\n",
    "\n",
    "'CatBoostEncoder()' is an encoding method that relies on the target to determine the values of the encoded features. It's basically an enhanced version of 'TargetEncoder()', which solves the data leakage problem that 'TargetEncoder()' suffers from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c54d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import CatBoostEncoder\n",
    "encoder = CatBoostEncoder()\n",
    "\n",
    "X[:,1:2] = encoder.fit_transform(X[:,1:2],y)\n",
    "X[:,2:3] = encoder.fit_transform(X[:,2:3],y)\n",
    "X[:,3:4] = encoder.fit_transform(X[:,3:4],y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5fdc7",
   "metadata": {},
   "source": [
    " We can notice that we gave the 'CatBoostEncoder()' two values: the column we need to encode and the target. The reason for that is that the 'CatBoostEncoder()' relies on the target of our dataset to encode the data.\n",
    "\n",
    "We can also notice that we got a warning in the previous code. It's stating that in a future version of Python, the new columns will be object-type, which is not a big issue since that can be fixed with a simple code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3aafe9",
   "metadata": {},
   "source": [
    "# 7- Model Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c400496",
   "metadata": {},
   "source": [
    "Now that we have solved all our problems with the dataset, we can start training our model. We can achieve better results by changing the values of 'test_size()' and 'random_state()'. To get the best results, it is best to try multiple values and run your model each time, compare the results of each operation, then choose the values which gave you the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "131f7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c26b4a",
   "metadata": {},
   "source": [
    "Note that we have to scale our 'X_train' and 'X_test' to use it with 'LogisticRegression()'. To do so, we will use 'sklearn.preprocessing.StandardScaler()', which scales our data based on all the values found in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9064509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4cae9",
   "metadata": {},
   "source": [
    "# 8- LogisticRegression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be94d184",
   "metadata": {},
   "source": [
    " For this model, we will apply 'LogisticRegression()'. Other algorithms can be used and would probably give better results (such as SVR), but since they take way too long to run and since 'LogisticRegression()' gives us decent results, we will use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "672c47c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regressor=LogisticRegression()\n",
    "\n",
    "regressor.fit(X_train,y_train)\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae99f3e",
   "metadata": {},
   "source": [
    "The warning in the previous code states that our algorithm reached the maximum number of iterations. We can solve that by changing the value of the parameter 'max_iter()' but that will make our code take way more time to run for a very small improvement in the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf5123",
   "metadata": {},
   "source": [
    "# 9- Result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27de11",
   "metadata": {},
   "source": [
    "To show the accuracy of our model, we can use the 'confusion_matrix()'. Note that our target has many unique numbers (23 to be precise), so the 'confusion_matrix()' - ironically - can be confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0df115ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   534,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,     17,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      1,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      4,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      3,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,     10,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      1,      2,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,   3110,      0,      0,\n",
       "             0,      0,      6,     19,      0,      0,      0,      1,\n",
       "             5,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      3,      0,\n",
       "             0,      0,      0,      3,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             2,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      1,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0, 268104,      5,      7,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      5,      0,      0,\n",
       "             0,      0,    478,     63,      0,      0,      0,      1,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [    57,      2,      0,      0,      0,     52,      2,      0,\n",
       "             1,     12,      4, 242955,      0,      1,      0,     24,\n",
       "            16,     23,      0,      0,     75,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      1,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      1,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      1,      0,      0,     64,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      2,      0,      0,\n",
       "             0,      2,      4,     36,      0,      0,      0,   2563,\n",
       "            19,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      4,      0,      0,\n",
       "             0,      0,      0,    144,      0,      0,      0,      6,\n",
       "          3797,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      6,      0,      0,\n",
       "             0,      0,      0,      9,      0,      0,      0,      0,\n",
       "            17, 701826,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      2,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      1,      0,\n",
       "             0,      0,      0,    258,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,    186,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,     47,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      3,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2a4c6",
   "metadata": {},
   "source": [
    "#### A better method to show the accuracy of our model if we have a large amount of unique values in our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fed37e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(round(accuracy_score(y_test, y_pred),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5cc105",
   "metadata": {},
   "source": [
    "#### Our model achieved an accuracy of 99.93%, which is marvelous!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
